{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Headers for HTTP requests\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = \"https://www.krimfup.se/search\"\n",
    "\n",
    "# Function to extract the total number of pages\n",
    "def get_total_pages(city):\n",
    "    url = f\"{base_url}?page=1&name=&ssn=&place={city}&crime=\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    pagination = soup.find('ul', class_='pagination')\n",
    "    if pagination:\n",
    "        return max([int(link.text) for link in pagination.find_all('a') if link.text.isdigit()])\n",
    "    return 1  # Default to 1 page if no pagination is found\n",
    "\n",
    "# Function to scrape names and cities\n",
    "def scrape_names_and_cities(city):\n",
    "    lst = []\n",
    "    last_page = get_total_pages(city)\n",
    "    print(f\"Total pages: {last_page}\")\n",
    "\n",
    "    for page in range(1, last_page + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}?page={page}&name=&ssn=&place={city}&crime=\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        containers = soup.select(\"div.table-responsive\")\n",
    "        for container in containers:\n",
    "            rows = container.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3:\n",
    "                    link = cells[0].find(\"a\")\n",
    "                    name = link.text.strip() if link else \"N/A\"\n",
    "                    location = cells[2].text.strip()\n",
    "                    if location.lower() == city.lower():\n",
    "                        lst.append([name, location])\n",
    "\n",
    "    return pd.DataFrame(lst, columns=[\"Name\", \"City\"])\n",
    "\n",
    "# Function to scrape Ratsit for addresses\n",
    "def scrape_ratsit(first_name, last_name, city):\n",
    "    url = f\"https://www.ratsit.se/sok/person?vem={first_name}%20{last_name}%20{city}&m=0&k=0&r=0&er=0&b=0&eb=0&amin=16&amax=120&fon=1&page=1\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        agree_button_selector = \"button#CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, agree_button_selector))\n",
    "            ).click()\n",
    "            print(\"Cookies accepterades.\")\n",
    "        except Exception as e:\n",
    "            print(\"Cookie-popup hittades inte eller kunde inte klickas:\", e)\n",
    "\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        elements = driver.find_elements(By.CSS_SELECTOR, \".pe-1.search-result-list-icon\")\n",
    "        results = [element.text for element in elements]\n",
    "        return results if results else [\"No address found\"]\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to convert address to latitude and longitude using Google API\n",
    "def geocode_address(address, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\"address\": address, \"key\": api_key}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"results\"]:\n",
    "            location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "            return location[\"lat\"], location[\"lng\"]\n",
    "    return None, None\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the city from user input\n",
    "    city = input(\"Enter city: \").strip()\n",
    "\n",
    "    # Google API key (replace with your actual key)\n",
    "    GOOGLE_API_KEY = \"AIzaSyBYkpdEMtQ3wiFl-plag65wvnZBOZo8ZRY\"\n",
    "\n",
    "    # Step 1: Scrape names and cities\n",
    "    df = scrape_names_and_cities(city)\n",
    "    df[\"First Name\"] = df[\"Name\"].apply(lambda x: x.split()[0] if len(x.split()) > 0 else None)\n",
    "    df[\"Last Name\"] = df[\"Name\"].apply(lambda x: x.split()[1] if len(x.split()) > 1 else None)\n",
    "    df[\"Address\"] = None\n",
    "    df[\"Lat\"] = None\n",
    "    df[\"Lon\"] = None\n",
    "\n",
    "    df = df.head(50)\n",
    "    # Step 2: Scrape Ratsit for addresses\n",
    "    for index, row in df.iterrows():\n",
    "        first_name = row[\"First Name\"]\n",
    "        last_name = row[\"Last Name\"]\n",
    "        city = row[\"City\"]\n",
    "\n",
    "        if first_name and last_name and city:\n",
    "            print(f\"Scraping address for {first_name} {last_name} in {city}...\")\n",
    "            results = scrape_ratsit(first_name, last_name, city)\n",
    "            df.at[index, \"Address\"] = results[0] if results else \"No address found\"\n",
    "\n",
    "    # Step 3: Geocode addresses\n",
    "    locations = []\n",
    "    for index, row in df.iterrows():\n",
    "        address = row[\"Address\"]\n",
    "        if address and address != \"No address found\":\n",
    "            print(f\"Geocoding address: {address}\")\n",
    "            lat, lon = geocode_address(address, \"AIzaSyBYkpdEMtQ3wiFl-plag65wvnZBOZo8ZRY\")\n",
    "            df.at[index, \"Lat\"] = lat\n",
    "            df.at[index, \"Lon\"] = lon\n",
    "            if lat and lon:\n",
    "                locations.append({\"Name\": row[\"Name\"], \"Address\": address, \"Lat\": lat, \"Lon\": lon})\n",
    "\n",
    "    # Step 4: Save results to JSON and CSV\n",
    "    with open(\"locations.json\", \"w\") as file:\n",
    "        json.dump(locations, file, indent=4)\n",
    "\n",
    "    df.to_csv(\"updated_dataframe_with_geocodes.csv\", index=False)\n",
    "\n",
    "    print(\"Updated DataFrame:\")\n",
    "    print(df)\n",
    "    print(\"Locations saved to locations.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m city \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter city: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Google API key (replace with your actual key)\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m GOOGLE_API_KEY \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGLE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Step 1: Scrape names and cities\u001b[39;00m\n\u001b[0;32m    102\u001b[0m df \u001b[38;5;241m=\u001b[39m scrape_names_and_cities(city)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Headers for HTTP requests\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = \"https://www.krimfup.se/search\"\n",
    "\n",
    "# Function to extract the total number of pages\n",
    "def get_total_pages(city):\n",
    "    url = f\"{base_url}?page=1&name=&ssn=&place={city}&crime=\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    pagination = soup.find('ul', class_='pagination')\n",
    "    if pagination:\n",
    "        return max([int(link.text) for link in pagination.find_all('a') if link.text.isdigit()])\n",
    "    return 1  # Default to 1 page if no pagination is found\n",
    "\n",
    "# Function to scrape names and cities\n",
    "def scrape_names_and_cities(city):\n",
    "    lst = []\n",
    "    last_page = get_total_pages(city)\n",
    "    print(f\"Total pages: {last_page}\")\n",
    "\n",
    "    for page in range(1, last_page + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}?page={page}&name=&ssn=&place={city}&crime=\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        containers = soup.select(\"div.table-responsive\")\n",
    "        for container in containers:\n",
    "            rows = container.find_all(\"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3:\n",
    "                    link = cells[0].find(\"a\")\n",
    "                    name = link.text.strip() if link else \"N/A\"\n",
    "                    location = cells[2].text.strip()\n",
    "                    if location.lower() == city.lower():\n",
    "                        lst.append([name, location])\n",
    "\n",
    "    return pd.DataFrame(lst, columns=[\"Name\", \"City\"])\n",
    "\n",
    "# Function to scrape Ratsit for addresses\n",
    "def scrape_ratsit(first_name, last_name, city):\n",
    "    url = f\"https://www.ratsit.se/sok/person?vem={first_name}%20{last_name}%20{city}&m=0&k=0&r=0&er=0&b=0&eb=0&amin=16&amax=120&fon=1&page=1\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        agree_button_selector = \"button#CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 3).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, agree_button_selector))\n",
    "            ).click()\n",
    "            print(\"Cookies accepterades.\")\n",
    "        except Exception as e:\n",
    "            print(\"Cookie-popup hittades inte eller kunde inte klickas:\", e)\n",
    "\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        elements = driver.find_elements(By.CSS_SELECTOR, \".pe-1.search-result-list-icon\")\n",
    "        results = [element.text for element in elements]\n",
    "        return results if results else [\"No address found\"]\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Function to convert address to latitude and longitude using Google API\n",
    "def geocode_address(address, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    params = {\"address\": address, \"key\": api_key}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"results\"]:\n",
    "            location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "            return location[\"lat\"], location[\"lng\"]\n",
    "    return None, None\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the city from user input\n",
    "    city = input(\"Enter city: \").strip()\n",
    "\n",
    "    # Google API key (replace with your actual key)\n",
    "    GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    # Step 1: Scrape names and cities\n",
    "    df = scrape_names_and_cities(city)\n",
    "\n",
    "    # Adjust the logic to extract first_name and last_name\n",
    "    df[\"First Name\"] = df[\"Name\"].apply(lambda x: x.split()[0] if len(x.split()) > 0 else None)\n",
    "    df[\"Last Name\"] = df[\"Name\"].apply(lambda x: x.split()[-1] if len(x.split()) > 1 else None)\n",
    "\n",
    "    df[\"Address\"] = None\n",
    "    df[\"Lat\"] = None\n",
    "    df[\"Lon\"] = None\n",
    "\n",
    "    df = df.head(2)  # Limit to 50 rows for testing\n",
    "\n",
    "    # Step 2: Scrape Ratsit for addresses\n",
    "    for index, row in df.iterrows():\n",
    "        first_name = row[\"First Name\"]\n",
    "        last_name = row[\"Last Name\"]\n",
    "        city = row[\"City\"]\n",
    "\n",
    "        if first_name and last_name and city:\n",
    "            print(f\"Scraping address for {first_name} {last_name} in {city}...\")\n",
    "            results = scrape_ratsit(first_name, last_name, city)\n",
    "            df.at[index, \"Address\"] = results[0] if results else \"No address found\"\n",
    "\n",
    "    # Step 3: Geocode addresses\n",
    "    locations = []\n",
    "    for index, row in df.iterrows():\n",
    "        address = row[\"Address\"]\n",
    "        if address and address != \"No address found\":\n",
    "            print(f\"Geocoding address: {address}\")\n",
    "            lat, lon = geocode_address(address, GOOGLE_API_KEY)\n",
    "            df.at[index, \"Lat\"] = lat\n",
    "            df.at[index, \"Lon\"] = lon\n",
    "            if lat and lon:\n",
    "                locations.append({\"Name\": row[\"Name\"], \"Address\": address, \"Lat\": lat, \"Lon\": lon})\n",
    "\n",
    "    # Step 4: Save results to JSON and CSV\n",
    "    with open(\"locations.json\", \"w\") as file:\n",
    "        json.dump(locations, file, indent=4)\n",
    "\n",
    "    df.to_csv(\"updated_dataframe_with_geocodes.csv\", index=False)\n",
    "\n",
    "    print(\"Updated DataFrame:\")\n",
    "    print(df)\n",
    "    print(\"Locations saved to locations.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
